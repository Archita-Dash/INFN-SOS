{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks with PyTorch\n",
    "> LPC Statistics Course 2021, Fermilab <br>\n",
    "> INFN SOS 2022, Paestum, Italy<br>\n",
    "> Harrison B. Prosper\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, we use a convolutional neural network (CNN) to classify the handwritten digits in the MNIST data set. State of the art neural network models achieve a prediction accuracy of __99.8\\%__. Remarkably, the simple model described here can reach __99\\%__ accuracy!\n",
    "\n",
    "Each image in the MNIST dataset is represented by a $(28, 28)$ matrix, with matrix element values in the set $[0, 1/255, \\cdots, 1]$. In practice, a __batch__ of images is stored in a 4-index tensor $\\mathbf{x}_{ncij}$ of __shape__ $(N, C, H, W)$. Think of this as a 4-dimensional array each cell of which contains a pixel value. The first index (dim = 0) labels the ordinal value (position) of an image in a batch of $N$ images; the second index (dim = 1) labels the number of __channels__, $C$, that is, image planes, which for a gray scale image is $C = 1$, and would be $C = 3$ for a red, green, blue (RGB) image, while the last two indices (dim = 2, 3) label the pixels of an image of height and width $H$ and $W$, respectively. \n",
    "\n",
    "### Model\n",
    "\n",
    "A typical convolutional neural network (CNN) comprises an alternating sequence of convolutional and coarse-graining (or __down-sampling__) layers ending with a fully connected feedforward neural network. A convolutional layer cross-correlates its inputs with a kernel, while (typically) increasing the number of output channels. The coarse-graining layer takes in one or more image planes, that is, channels, and creates multi-channel output images of smaller size. Here is a high-level view of the model we shall fit,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} & = \\mbox{softmax}(\\mathbf{b}_2 + {\\rm fn} (\\mathbf{w}_2, \\mathbf{b}_1 + {\\rm cc}(\\mathbf{w}_1, \\, \\mathbf{b}_0 + {\\rm cc}(\\mathbf{w}_0, \\, \\mathbf{x}) \\, ) \\,)\\,),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{b}$ and $\\mathbf{w}$, the biases and weights, are the parameters of the model and the functions __softmax__, __fn__, and __cc__ are defined as follows:\n",
    "\n",
    "  * __softmax__ For $K$ output classes, \n",
    "\\begin{align*}\n",
    "\\mbox{softmax}(x)_k & = \\frac{\\exp(x_{k})}{\\sum_{j=1}^{K} \\exp(x_{j})} ,\n",
    "\\end{align*}\n",
    "where $x_{k}$ denotes the $k^\\mbox{th}$ output of the previous layer. The\n",
    "softmax function\n",
    "bounds the output values to the unit interval and ensure that \n",
    "their sum is unity. \n",
    "\n",
    "  * __fn__ Given tensors $\\mathbf{w}$ and $\\mathbf{x}$, this function operates element-wise on its inputs and returns \n",
    "  \n",
    "  $$\\text{fn}(\\mathbf{w}, \\mathbf{x}) = \\mathbf{w} \\, \\text{flatten}(\\, \\text{relu}(\\,\\text{maxpool}(\\,\\mathbf{x}\\,)\\,)\\,),$$ where flatten, relu, and maxpool are defined below.\n",
    "\n",
    "\n",
    "  * __cc__ Given tensors $\\mathbf{w}$ and $\\mathbf{x}$, this function returns their cross-correlation.\n",
    "\n",
    "    $$\\mathbf{x}_{kij} = \\sum_{c=1}^{C} \\sum_{r=-2}^2 \\sum_{s=-2}^2  \\mathbf{x}_{c,\\, i+r,\\,j+s} \\, \\mathbf{w}_{kcrs} .$$\n",
    "\n",
    "  * __flatten__ Given tensor $\\mathbf{x}$, this function restructures $\\mathbf{x}$ into a 1D tensor (a 1D array).\n",
    "\n",
    "  * __relu__ Given tensor $\\mathbf{x}$, the function \n",
    "\\begin{align*}\n",
    "{\\rm relu}(x) &= \\begin{cases}\n",
    "    x, & \\text{if } x \\gt 0\\\\\n",
    "    0              & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "is applied\n",
    "*element-wise*, that is, to every element of the tensor.\n",
    "\n",
    "  * __maxpool__ Given tensor $\\mathbf{x}$, this function uses a moveable window to apply a coarse-graining operation over $\\mathbf{x}$. In effect, the moveable window  splits the tensor into non-overlapping pieces. For each piece, the maximum pixel value within that piece is returned. For example, if $\\mathbf{x}$ is a 2D tensor, e.g., a matrix, of shape (28, 28), and the window is of shape (2, 2), maxpool splits the original matrix into non-overlapping pieces of shape (2,2) and returns the maximum value within each piece, thereby creating a new matrix of shape (14, 14).\n",
    "\n",
    "\n",
    "A high-level view is a good way to represent the structure of a model.  But, it is also good to try to understand the details...at least once. As is often true, a detailed mathematical description of a complicated function is made easier using an example. \n",
    "\n",
    "#### Convolutional layer\n",
    "The data into the first convolutional layer are a sequence of single channel (gray scale) images $\\mathbf{x}$ of shape $(N, 1, 28, 28)$, that is, the data  form a tensor comprising $N$ images, each with a single channel ($C = 1)$, and each a $(28, 28)$ matrix of pixels. This tensor is cross-correlated with a 4D tensor of weights $\\mathbf{w}_0$, with shape $(4, 1, 5, 5)$. We shall refer to the latter as a __kernel__ and each of its $4 \\times 1$, $(5, 5)$, matrices (or 2D tensors) as a __filter__. The first index of the kernel is the number of __output channels__ (here, 4), the second is the number of __input channels__ (here, 1). Therefore, a given $(5, 5)$ filter is identified by two indices: its output and input channel numbers. The last two indices of the kernel are the height (5) and width (5), respectively, of its filters. \n",
    "\n",
    "(Unfortunately, the jargon is not consistent across the machine learning literature. What we call a filter is indeed often referred to as a filter, but it is also called a kernel, and what we call a kernel, that is, a stack of one or more filters, often has no specific name attached to it!) \n",
    "\n",
    "The operation of the convolutional layer for filters with odd-numbered height and width, e.g. $(5, 5)$, can be written as\n",
    "\n",
    "\\begin{align}\n",
    "    (\\mathbf{b}_0)_k + (\\mathbf{w}_0 \\otimes \\mathbf{x})_{nkij} &= (\\mathbf{b}_0)_k + \\sum_{c=0}^{C-1} \\sum_{r=-2}^2 \\sum_{s=-2}^2  \\mathbf{x}_{nc,\\, i+r,\\,j+s} \\, (\\mathbf{w}_0)_{kcrs}, \\quad i, j = 0,\\cdots, 27 ,\n",
    "\\end{align}\n",
    "\n",
    "where, we have chosen to label the central matrix element of a filter with the indices $(0, 0)$ and the other elements with indices that can be positive or negative integers. In the above expression, for a given pixel $i, j$, the filter labeled $k, c$, namely the matrix $(\\mathbf{w}_0)_{kc}$ with matrix elements $(\\mathbf{w}_0)_{kcrs}$, is cross-correlated with channel $c$ of input image $n$ and a sum over the cross-correlated input channels, that is, over the index $c$, is performed, resulting in $k$ output images.\n",
    "\n",
    "However, for the above computations to work as shown, we need (in effect) to surround each image with extra strips of pixels. This operation is called __padding__. In the following, we choose to pad each image with 2 strips of pixels, with value zero. This padding makes it possible to center the $(5, 5)$ filters on every pixel, $i, j$, of the image. Had we used, for example, $(3, 5)$ filters, the appropriate padding shape would be $(1, 2)$, that is, 1 extra strip of pixels along the top and bottom of the image and 2 extra strips along the left and right sides of the image. We are free to make other choices.\n",
    "\n",
    "Since the input image is a $(28, 28)$ matrix padded with 2 strips of pixels around the boundary, and each filter is a $(5, 5)$ matrix,\n",
    "the output of the convolution step will be a tensor of shape $(N, 4, 28, 28)$ provided that we use a __stride__ of 1, that is, if during the cross-correlation, we shift the filters a horizontal or vertical step of 1 pixel. Other choices are possible. By\n",
    "construction, the output of this convolutional layer is $N$, 4-channel images, of the same size as the original.\n",
    "\n",
    "#### Coarse-graining layer\n",
    "A convolutional layer is usually followed by a coarse-graining, that is, down-sampling, layer that reduces the number of pixels per image. In this example, the number of pixels in the image created by the convolutional layer is reduced by a factor 2 in both directions; that is, each of the 4 channels of the $(28, 28)$ image is down-sampled into a $(14, 14)$ channel by replacing the corresponding group of pixels of shape $(2, 2)$ with a pixel whose value is set to the largest value among the group of 4 pixels.  This operation, called __max-pooling__, can be expressed as\n",
    "\n",
    "\\begin{align}\n",
    "    {\\rm maxpool}(O)_{nkij} &= \\max{ \\{O_{nk,\\, 2i + r,\\, 2j + s} \\}_{r,\\,s = 0, 1} } \\quad i, j = 0,\\cdots, 13,\n",
    "\\end{align}\n",
    "\n",
    "where $O$ is the output of a convolutional layer. Again, other choices are possible.\n",
    "\n",
    "After max-pooling, the output tensor has shape $(N, 4, 14, 14)$. A relu __activation function__ is applied to each element of this tensor and completes one sequence of operations: cross-correlation, down-sampling, and activation. \n",
    "\n",
    "A second convolutional layer follows whose input must match the output of the previous max-pooling operation, which contains $N$ sequences of images, each with 4 channels, and each a $(14, \\, 14)$ matrix padded with a 2-pixel boundary. The kernel in this layer contains $16 \\times 4$, $(5, 5)$, filters, that is, it is of shape $(16, 4, 5, 5)$. For a given input channel $c$, each of the 16 $(5, 5)$ filters of the sub-kernel $(16, c, 5, 5)$ is cross-correlated with input channel $c$. The cross-correlated input channels are then summed pixel by pixel over the 4 channels to yield  a 16-channel output image from this layer. The output, therefore, is a tensor of shape $(N, 16, 14, 14)$.\n",
    "\n",
    "Finally, a second max-pooling layer is applied, which transforms the 16-channel image of 2D size $(14, 14)$ to a 16-channel image of 2D size $(7, 7)$. The image is then flattened into a 1D tensor (basically, a 1D array) of size $16 \\times 7 \\times 7 = 784$, which are the inputs of the final linear layer with 10 outputs. Finally, a softmax function is applied.\n",
    "\n",
    "\n",
    "### Discussion\n",
    "The net effect of the sequence of nested layers, prior to flattening, is to transform a multi-channel input image into a multi-channel output image. The hope is that the output image captures the most relevant features of the input image thereby improving the accuracy of the subsequent classification. But, why use cross-correlation?\n",
    "\n",
    "The intuition behind the cross-correlation operation is that natural images tend to have similar features in different parts of the image. A filter that is sensitive to vertical features would tend to produce an output image that enhances vertical features, while suppressing the expression of horizontal features. It therefore seems plausible that if one could design filters that are sensitive to different image features, the set of filters could potentially transform an image into another in which the most relevant features are enhanced while the least relevant are suppressed. \n",
    "\n",
    "In the early days of image recognition systems, researchers tried to describe in software what they considered to be the most relevant features in images. This proved to be extremely difficult and success was limited. Today hand-coding is no longer necessary because the values of the parameters of the kernels can be determined automatically by minimizing an appropriate (average) loss function. However, the mere fact that a large number of images is needed to fit these parameters is evidence that current machine learning methods and models, while spectacularly successful, are still far removed from human learning methods and models. \n",
    "\n",
    "A young child need be shown labeled objects, cat, dog, doll, truck, car, just a few times before being able to classify them correctly. The child can do this even when the objects are presented to her in orientations that differ from the ones used during the \"training phase\". This extraordinary ability suggests that the child's brain constructs a fault tolerant model of each object using only its most relevant features and is able to match rapidly the input image with the stored, or perhaps auto-reconstructed, fault tolerant model. The model is clearly fault tolerant because the child is able to ignore \"faults\" such as viewing the objects in lighting that differs from that used during the \"training phase\", or ignoring the fact that now the car has lost a couple of wheels or the doll is now wearing different clothes and so on. \n",
    "\n",
    "In spite of the truly impressive recent successes, it is clear that machine learning-based artificial intelligence still has a long way to go. Of course, it could be that the age-old goal of building machines that think like humans is simply the wrong goal. After all, there are already many instances in which researchers have inadvertently succeeded in producing machines that mimic our prejudices, which is hardly progress. Therefore, perhaps we should embrace the \"artificial\" in artificial intelligence and the possibility that these machines will inevitably \"think\" differently from us. The trick, of course, is to have them \"think\" in a way that improves the human condition. It would not be helpful if, for example, our super-intelligent machine concluded that the best way to solve climate change is to eliminate one of its culprits, namely, the species Homo sapiens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to plot pixelized images\n",
    "import imageio as im\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# to load objects into memory\n",
    "import joblib as jb\n",
    "\n",
    "# to reload modules\n",
    "import importlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update fonts\n",
    "FONTSIZE = 14\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "mp.rc('text', usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility \n",
    "# on a given machine\n",
    "seed = 128\n",
    "rnd  = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "Train_x, Train_t = jb.load('../datasets/mnist_train.pkl.gz')\n",
    "test_x,  test_t  = jb.load('../datasets/mnist_test.pkl.gz')\n",
    "print(len(Train_x), len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few  images\n",
    "Use imshow(..) and show() to display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(x, n_rows=2, n_cols=2, f_size=(5, 5)):\n",
    "    f, ax = plt.subplots(nrows=n_rows, \n",
    "                         ncols=n_cols, \n",
    "                         figsize=f_size)\n",
    "    \n",
    "    # note use of flatten() to convert a matrix of shape (nrows, ncols)\n",
    "    # to a 1-d array.\n",
    "    d = np.arange(0.5, 28.05, 1)\n",
    "    for image, ax in zip(x, ax.flatten()):\n",
    "        #ax.axis('off')\n",
    "        ax.tick_params(axis='x', colors='white')\n",
    "        ax.tick_params(axis='y', colors='white')\n",
    "\n",
    "        ax.imshow(image.reshape(28, 28), cmap='gray')\n",
    "\n",
    "        ax.set_xticks(d, minor=False)\n",
    "        ax.set_yticks(d, minor=False)\n",
    "        ax.xaxis.grid(True, which='major')\n",
    "        ax.yaxis.grid(True, which='major')\n",
    "\n",
    "    plt.savefig('mnist_images.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotImages(Train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images for training:        55000\n",
      "number of images for validation:       5000\n",
      "number of images for testing:         10000\n"
     ]
    }
   ],
   "source": [
    "M = 55000\n",
    "train_x, val_x = Train_x[:M], Train_x[M:]\n",
    "train_t, val_t = Train_t[:M], Train_t[M:]\n",
    "\n",
    "n_train = len(train_x)\n",
    "n_valid = len(val_x)\n",
    "n_test  = len(test_x)\n",
    "\n",
    "print(\"number of images for training:   %10d\" % n_train)\n",
    "print(\"number of images for validation: %10d\" % n_valid)\n",
    "print(\"number of images for testing:    %10d\" % n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer normalization\n",
    "\n",
    "We shall use layer normalization to ensure that inputs into every layer are approximately bounded and are of order unity. (See https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    y \\leftarrow \\frac{y - \\text{E}(y)}{\\sqrt{\\text{Var}(y) + \\epsilon}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Here is a simple example of the use of LayerNorm. Note: even though it is intended to normalize layers, as in the example below, LayerNorm can be used to normalize any tensor. This example comprises two 2-channel \"images\". We normalize each of the 2-channel images of shape (C=2, H=3, W=3), separately, and compute the mean and variances before and after normalization both \"by hand\" and using the PyTorch methods mean and var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3, 3])\n",
      "\n",
      "Original tensor\n",
      "\n",
      "tensor([[[[ 4.5549,  6.1408,  5.6103],\n",
      "          [ 2.2630,  2.4559,  6.3304],\n",
      "          [ 5.0636,  6.4812,  9.9371]],\n",
      "\n",
      "         [[ 9.4994,  0.6406,  8.5266],\n",
      "          [ 6.5659,  0.8795,  3.2504],\n",
      "          [ 9.6275,  0.2997,  3.6203]]],\n",
      "\n",
      "\n",
      "        [[[ 4.3823,  1.5339, 16.6906],\n",
      "          [15.8431,  8.3686,  5.8599],\n",
      "          [19.9799,  0.7314,  4.9350]],\n",
      "\n",
      "         [[ 4.1297,  4.6083, 15.1368],\n",
      "          [17.6758,  3.0346, 11.7540],\n",
      "          [19.8771, 13.8926,  2.6613]]]])\n",
      "\n",
      "mean:     5.10, variance:     9.09\n",
      "from PyTorch - mean: tensor(5.0971), variance: tensor(9.0886)\n",
      "\n",
      "mean:     9.51, variance:    43.09\n",
      "from PyTorch - mean: tensor(9.5053), variance: tensor(43.0938)\n",
      "\n",
      "Normalized tensor\n",
      "\n",
      "tensor([[[[-0.1798,  0.3462,  0.1703],\n",
      "          [-0.9401, -0.8761,  0.4091],\n",
      "          [-0.0111,  0.4591,  1.6054]],\n",
      "\n",
      "         [[ 1.4603, -1.4782,  1.1376],\n",
      "          [ 0.4872, -1.3990, -0.6125],\n",
      "          [ 1.5028, -1.5913, -0.4898]]],\n",
      "\n",
      "\n",
      "        [[[-0.7804, -1.2143,  1.0945],\n",
      "          [ 0.9655, -0.1732, -0.5553],\n",
      "          [ 1.5956, -1.3365, -0.6962]],\n",
      "\n",
      "         [[-0.8189, -0.7460,  0.8579],\n",
      "          [ 1.2446, -0.9857,  0.3426],\n",
      "          [ 1.5800,  0.6683, -1.0426]]]])\n",
      "\n",
      "mean:     0.00, variance:     1.00\n",
      "from PyTorch - mean: tensor(3.3114e-08), variance: tensor(1.0000)\n",
      "\n",
      "mean:    -0.00, variance:     1.00\n",
      "from PyTorch - mean: tensor(-5.9605e-08), variance: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# create a (2,2,3,3) tensor\n",
    "# ---------------------------------------\n",
    "N, C, H, W = 2, 2, 3, 3\n",
    "x = torch.randn(N, C, H, W) * 0 # zero tensor\n",
    "for n in range(N):\n",
    "    for c in range(C):\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                x[n,c,h,w] = np.random.uniform(0, 10*(1+n))\n",
    "print(x.shape)\n",
    "print('\\nOriginal tensor\\n')\n",
    "print(x)\n",
    "\n",
    "# compute mean and variance over tensor elements [C, H, W]\n",
    "def mean_var(x):\n",
    "    N, C, H, W = x.shape\n",
    "    x1 = np.zeros(N)\n",
    "    x2 = np.zeros(N)\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    v = x[n,c,h,w]\n",
    "                    x1[n] += v\n",
    "                    x2[n] += v*v\n",
    "    K   = C*H*W\n",
    "    x1 /= K\n",
    "    x2 /= K\n",
    "    x2  = x2-x1*x1\n",
    "    return x1, x2\n",
    "\n",
    "x1, x2 = mean_var(x)\n",
    "\n",
    "for n in range(N):\n",
    "    print('\\nmean: %8.2f, variance: %8.2f' % (x1[n], x2[n]))\n",
    "    \n",
    "    # compare with mean and variance from PyTorch\n",
    "    m = torch.mean(x[n])\n",
    "    v = torch.var(x[n], unbiased=False)\n",
    "    print('from PyTorch - mean: %s, variance: %s' % (m, v))\n",
    "    \n",
    "# ---------------------------------------\n",
    "# Normalize layers over elements [C, H, W]\n",
    "# ---------------------------------------\n",
    "nm = nn.LayerNorm([C, H, W], elementwise_affine=False)\n",
    "y  = nm(x) \n",
    "print('\\nNormalized tensor\\n')\n",
    "print(y)\n",
    "y1, y2 = mean_var(y)\n",
    "\n",
    "for n in range(N):\n",
    "    print('\\nmean: %8.2f, variance: %8.2f' % (y1[n], y2[n]))\n",
    "    m = torch.mean(y[n])\n",
    "    v = torch.var(y[n], unbiased=False)\n",
    "    print('from PyTorch - mean: %s, variance: %s' % (m, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model as a Python class\n",
    "\n",
    "The __nn.Sequential__ class is the simplest way to construct (simple) PyTorch models. But if you want more control over the model, you can achieve this by building the model as a class inheriting from __nn.Module__ as in the following cell. \n",
    "\n",
    "We use some notebook magic to write the following cell to a file, which can then be imported into another notebook. Note: we need to import the saved class to make it visible to this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CNN.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CNN.py\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "    A convolutional + fully-connected network for the MNIST\n",
    "    problem.\n",
    "        \"\"\" \n",
    "        # initialize base (or super, or parent) class\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # LAYER 0\n",
    "        # ----------------------------------------------------------\n",
    "        # since we are using 5x5 filters, the addition of 2-pixel\n",
    "        # padding around each image will allow cross-correlation of \n",
    "        # every pixel in the image, and every filter will be centered \n",
    "        # on each pixel as the filters are moved around the image. \n",
    "        # Since every pixel is processed, we obtain output images \n",
    "        # that are the same size as the input images.\n",
    "        self.conv0 = nn.Conv2d(in_channels=1,    # input channels\n",
    "                               out_channels=4,   # output channels\n",
    "                               kernel_size=5,    # filter size (5x5)\n",
    "                               stride=1,         # shift by this amount     \n",
    "                               padding=2)        # pad by this amount\n",
    "           \n",
    "        # normalize each image; that is, normalize over all numbers\n",
    "        # defined by the tensor indices (C=4, H=28, W=28).\n",
    "        self.layernorm0= nn.LayerNorm(normalized_shape=(4, 28, 28))\n",
    "\n",
    "        # down-sample with a (2,2) window (kernel_size), which shifts \n",
    "        #    horizontally or vertically with a stride of 2 pixels.\n",
    "        #    This operation replaces the 4 channels of shape (28, 28) \n",
    "        #    with 4 channels of shape (14, 14) by replacing a group of \n",
    "        #    2x2 pixels in an input channel with a single pixel whose \n",
    "        #    value equals the largest pixel value among the 4 pixels \n",
    "        #    within the (2, 2) window.\n",
    "        self.maxpool0  = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        \n",
    "        # apply a relu non-linearity to every element of the tensor\n",
    "        # input\n",
    "        self.relu0     = nn.ReLU()\n",
    "                \n",
    "        # LAYER 1\n",
    "        # ----------------------------------------------------------\n",
    "        # instantiate a 2nd convolution layer\n",
    "        # Note: the in_channels count must match the out_channels \n",
    "        # count of the previous layer.\n",
    "        self.conv1 = nn.Conv2d(in_channels=4,    # input channels\n",
    "                               out_channels=16,  # output channels\n",
    "                               kernel_size=5,\n",
    "                               stride=1,\n",
    "                               padding=2)\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=(16, 14, 14))\n",
    "        self.maxpool1   = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.relu1      = nn.ReLU()\n",
    "        \n",
    "        # we end with a linear layer. Let's compute the number of\n",
    "        # inputs to that layer. Ignoring the first index of the output\n",
    "        # tensor from conv0 and conv1, which simply labels the ordinal \n",
    "        # value of the image in the batch of images, we note the \n",
    "        # following:\n",
    "        # 1. conv0 outputs a tensor of size (4, 28, 28), that is, it\n",
    "        #    outputs batches of 4 channels of shape (28, 28).\n",
    "        #    we then down-sample to a tensor of size (4, 14, 14).\n",
    "        #\n",
    "        # 2. conv1 outputs a tensor of size (16, 14, 14), which will be\n",
    "        #    down-sampled to one of size (16, 7, 7).\n",
    "        #\n",
    "        # 3. Therefore, when flattened, the number of inputs to the\n",
    "        #    linear layer is: 16 * 7 * 7 = 784 (which, just happens to\n",
    "        #    equal the number of pixels in the original images!)\n",
    "        self.n_inputs = 16 * 7 * 7\n",
    "        \n",
    "        # 4. We have 10 outputs, one for each digit\n",
    "        self.linear   = nn.Linear(self.n_inputs, 10)\n",
    "\n",
    "        # see description in forward(...) method\n",
    "        self.dropout  = nn.Dropout(p=0.5)\n",
    "\n",
    "        # see description in forward(...) method\n",
    "        self.softmax  = nn.Softmax(dim=1)\n",
    "    \n",
    "    # define (required) method to compute output of network\n",
    "    def forward(self, x):\n",
    "        # conv0 expects a 4-d tensor of shape \n",
    "        # (N=batch_size, C=channels, H=height, W=width). So we must \n",
    "        # reshape x. The -1 index means the batch size is to be\n",
    "        # inferred at runtime from the tensor x.\n",
    "        y = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # LAYER 0\n",
    "        # 1. cross-correlate the input tensor of shape (-1, 1, 28, 28),\n",
    "        #    padded with a 2-pixel wide strip, with a (4, 1, 5, 5) \n",
    "        #    kernel, thereby producing an output tensor of shape \n",
    "        #    (-1, 4, 28, 28).\n",
    "        y = self.conv0(y)\n",
    "        \n",
    "        # 2. normalize images.\n",
    "        y = self.layernorm0(y)\n",
    "        \n",
    "        # 3. down-sample with a (2,2) window, which shifts horizontally\n",
    "        #    or vertically 2 pixels at a time. This replaces the 4 \n",
    "        #    channels of shape (28, 28) with 4 channels of shape (14, 14) \n",
    "        #    by replacing a group of 2x2 pixels in an input channel with \n",
    "        #    the largest pixel value among the 4 pixels. The output tensor \n",
    "        #    at this stage has shape (-1, 4, 14, 14).\n",
    "        y = self.maxpool0(y)\n",
    "        \n",
    "        # 4. apply a relu non-linearity to every element of this tensor\n",
    "        y = self.relu0(y)\n",
    "        \n",
    "        # LAYER 1\n",
    "        # 1. cross-correlate a (-1, 4, 14, 14) tensor, padded as above, \n",
    "        #    with a (16, 4, 5, 5) kernel and, for each (5, 5) filter, \n",
    "        #    sum over the 4 input channels. Since the kernel contains \n",
    "        #    16 output channels, the end result is a 16-channel image. \n",
    "        #    The output, therefore, has shape (-1, 16, 14, 14).\n",
    "        y = self.conv1(y) \n",
    "        \n",
    "        # 2. normalize the 16-channel image of shape (14, 14)\n",
    "        y = self.layernorm1(y)\n",
    "        \n",
    "        # 3. down-sample with a (2,2) window, as above, thereby \n",
    "        #    creating an output tensor of shape (-1, 16, 7, 7).\n",
    "        y = self.maxpool1(y)\n",
    "        \n",
    "        # 4. apply a relu function element-wise (as above).\n",
    "        y = self.relu1(y)\n",
    "       \n",
    "        # flatten the tensor (-1, 16, 7, 7) to the tensor (-1, 16*7*7).\n",
    "        y = y.view(-1, self.n_inputs)\n",
    "        \n",
    "        # During training, randomly dropout, that is, zero, \n",
    "        # half of the elements in the current tensor y. Dropout has\n",
    "        # been shown to reduce the tendency to overtrain.\n",
    "        # Dropout effectively deactivates all the weights attached \n",
    "        # to the zeroed element. Alternatively, it can be thought of \n",
    "        # as a way to apply random modifications to a multi-channel \n",
    "        # image by randomly setting half the pixels to zero at each \n",
    "        # iteration.\n",
    "        if self.training:\n",
    "            y = self.dropout(y)\n",
    "            \n",
    "        # Apply a linear transformation to the (-1, 784) tensor.\n",
    "        # We could use more than one linear layer here, which may\n",
    "        # (or may not!) yield better results.\n",
    "        y = self.linear(y)\n",
    "        \n",
    "        # Apply the softmax function horizontally, i.e., along \n",
    "        # the class axis (dim=1) in order to ensure that the outputs \n",
    "        # sum to unity.\n",
    "        # (Note: dim=0 is vertical, that is, along the batch axis.)\n",
    "        \n",
    "        # Final output: estimated class probabilities for ith image,\n",
    "        #   q_i(k) = exp(y_i(k) / sum_j exp(y_i(j)), j = 0,..., K-1,\n",
    "        # where K=10 is the number of classes and y_i(k) is the output \n",
    "        # for the ith image for class index k. \n",
    "        y = self.softmax(y)\n",
    "\n",
    "        return y\n",
    "    \n",
    "# Here is a much simpler way to implement the same model!\n",
    "\n",
    "model = nn.Sequential(nn.Conv2d(in_channels=1,   # input channels\n",
    "                                out_channels=4,  # output channels\n",
    "                                kernel_size=5,   # 5x5 filter size\n",
    "                                stride=1,\n",
    "                                padding=2),\n",
    "                      nn.LayerNorm(normalized_shape=(4, 28, 28)),\n",
    "                      nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "                      nn.ReLU(),\n",
    "                      \n",
    "                      nn.Conv2d(in_channels=4,   # input channels\n",
    "                                out_channels=16, # output channels\n",
    "                                kernel_size=5,\n",
    "                                stride=1,\n",
    "                                padding=2),\n",
    "                      nn.LayerNorm(normalized_shape=(16, 14, 14)),\n",
    "                      nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "                      nn.ReLU(),\n",
    "        \n",
    "                      nn.Flatten(),\n",
    "                      nn.Linear(784, 10),\n",
    "                      nn.Dropout(p=0.5),\n",
    "                      nn.Softmax(dim=1) \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CNN\n",
    "importlib.reload(CNN);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        outputs:  shape (batch_size, number_classes), pre-softmax outputs\n",
    "        targets:  shape (batch_size, ), class indices [0,...,C-1] \n",
    "        \"\"\"        \n",
    "        # ---------------------------------------------------------------       \n",
    "        # The cross entropy is defined by\n",
    "        #   H(p, q) = -sum_i p_i log(q_i)\n",
    "        #\n",
    "        # The entropy is defined by\n",
    "        #   H(p)    = -sum_i p_i log(p_i)\n",
    "        #\n",
    "        # and the Kullback-Leibler divergence by\n",
    "        #   D(p||q) = -sum_i p_i log(p_i/q_i)\n",
    "        #           = -sum_i p_i log(p_i) + sum_i p_i log(q_i)\n",
    "        #           = H(p) - H(p, q)\n",
    "        #\n",
    "        # The cross entropy is minimized when the estimated \n",
    "        # probabilities q_i match the true probabilities p_i, in which \n",
    "        # case the cross entropy equals the entropy.\n",
    "        # --------------------------------------------------------------- \n",
    "        # Note the numpy-like syntax for accessing elements of the\n",
    "        # tensor: outputs[list1, list2] \n",
    "        # Note also: range(batch_size) is [0,...batch_size-1]\n",
    "        #\n",
    "        # outputs is a (-1, 10) tensor. For each image pick the output \n",
    "        # value corresponding to its class. Recall that targets is \n",
    "        # a 1D tensor (basically, a 1D array) of class labels. \n",
    "        # For every row, we pick the value in the column corresponding\n",
    "        # to the class label.\n",
    "        batch_size = len(outputs)\n",
    "        outputs    = outputs[range(batch_size), targets]\n",
    "        return -torch.mean(torch.log(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avloss = AverageLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batch\n",
    "Get a random sample from the training set of size $batch\\_size$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomBatch(x, t, batch_size):\n",
    "    # the numpy function choice(length, number)\n",
    "    # selects at random \"number\" integers from \n",
    "    # the range [0, length-1] \n",
    "    rows    = rnd.choice(len(x), batch_size)\n",
    "    # pick out rows defined by the integers in array \"rows\"\n",
    "    batch_x = x[rows]\n",
    "    batch_t = t[rows]\n",
    "    return (batch_x, batch_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Fraction of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    # For each image, return its predicted class label using argmax.\n",
    "    #\n",
    "    # argmax scans the numpy along the specified axis, here the \n",
    "    # horizontal axis, which is in the class direction, and returns the\n",
    "    # ordinal value of the maximum value, which is the predicted class. \n",
    "    # Note: outputs must be converted from a tensor to a numpy array \n",
    "    # before being passed to argmax. axis=1 is to numpy what dim=1 is\n",
    "    # to PyTorch.\n",
    "    outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "    \n",
    "    # count how often the predicted class matches the actual class and\n",
    "    # compute the fraction of correct predictions. \n",
    "    # Note: targets must be converted to a numpy array for the \n",
    "    # comparison to work since outputs is a numpy array.\n",
    "    return float(np.mean(outputs==targets.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_x, train_t, batch_size):\n",
    "    model.train() # training mode\n",
    "    \n",
    "    # get a random sequence of training data\n",
    "    batch_x, batch_t = randomBatch(train_x, train_t, batch_size)\n",
    "\n",
    "    # convert data from numpy arrays to tensors\n",
    "    with torch.no_grad():  # no need to compute gradients wrt. to x, t\n",
    "        x = torch.from_numpy(batch_x)\n",
    "        x = x.view(-1, 1, 28, 28)     # change to shape (N, C, H, W)\n",
    "        t = torch.from_numpy(batch_t)       \n",
    "    \n",
    "    # compute output of model\n",
    "    y = model(x)\n",
    "    \n",
    "    # compute loss functions, given the model outputs and the targets\n",
    "    loss    = loss_fn(y, t)        \n",
    "    optimizer.zero_grad()  # clear previous gradients\n",
    "    loss.backward()        # compute gradients\n",
    "    optimizer.step()       # move one step\n",
    "        \n",
    "def validate(model, train_x, train_t, val_x, val_t):\n",
    "    n_val = len(val_x)\n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    with torch.no_grad():  # no need to compute gradients wrt. to x, t\n",
    "        \n",
    "        batch_x, batch_t = randomBatch(train_x, train_t, n_val)\n",
    "        x = torch.from_numpy(batch_x).view(-1, 1, 28, 28)\n",
    "        t = torch.from_numpy(batch_t)       \n",
    "        y = model(x)\n",
    "        acc_t = accuracy(y, t)\n",
    "          \n",
    "        batch_x, batch_t = randomBatch(val_x, val_t, n_val)  \n",
    "        x = torch.from_numpy(batch_x).view(-1, 1, 28, 28)\n",
    "        t = torch.from_numpy(batch_t)      \n",
    "        y = model(x)\n",
    "        acc_v = accuracy(y, t)\n",
    "\n",
    "    return (acc_t, acc_v)\n",
    "               \n",
    "def trainModel(model, optimizer, averageloss, modelfile,\n",
    "               train_x, train_t,\n",
    "               val_x, val_t,\n",
    "               n_iterations, batch_size, \n",
    "               xx=[], yy_t=[], yy_v=[], step=100):\n",
    "\n",
    "    max_acc_v = -1e30\n",
    "    print('%10s\\t%10s\\t%10s' % ('iteration', 'training', 'validation'))\n",
    "    \n",
    "    for ii in range(n_iterations):\n",
    "        train(model, optimizer, averageloss, \n",
    "              train_x, train_t, \n",
    "              batch_size)\n",
    "\n",
    "        if ii  < 5:\n",
    "            acc_t, acc_v = validate(model, \n",
    "                                    train_x, train_t, \n",
    "                                    val_x, val_t)\n",
    "            \n",
    "            print(\"%10d\\t%10.4f\\t%10.4f\" % (ii, acc_t, acc_v))\n",
    "            \n",
    "            if len(xx) == 0:\n",
    "                xx.append(0)\n",
    "            else:\n",
    "                xx.append(xx[-1]+step)\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "\n",
    "        elif ii % step == 0:\n",
    "            acc_t, acc_v = validate(model, \n",
    "                                    train_x, train_t, \n",
    "                                    val_x, val_t)\n",
    "            \n",
    "            print(\"\\r%10d\\t%10.4f\\t%10.4f\" % (ii, acc_t, acc_v), end='')\n",
    "\n",
    "            xx.append(xx[-1]+step)\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            \n",
    "        if acc_v > max_acc_v:\n",
    "            max_acc_v = acc_v\n",
    "            torch.save(model.state_dict(), modelfile)\n",
    "            \n",
    "    print()\n",
    "    return (xx, yy_t, yy_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the model and choose minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (1): LayerNorm((4, 28, 28), eps=1e-05, elementwise_affine=True)\n",
      "  (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(4, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (5): LayerNorm((16, 14, 14), eps=1e-05, elementwise_affine=True)\n",
      "  (6): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): ReLU()\n",
      "  (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  (9): Linear(in_features=784, out_features=10, bias=True)\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Softmax(dim=1)\n",
      ")\n",
      "number of parameters: 22114\n"
     ]
    }
   ],
   "source": [
    "model = CNN.model\n",
    "print(model)\n",
    "print('number of parameters:', number_of_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!\n",
    "\n",
    "During training, the model parameters are saved to __mnist_cnn.dict__ every time the current best prediction accuracy, using the validation data, is exceeded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = 'mnist_cnn.dict'\n",
    "xx   = []  # iteration numbers\n",
    "yy_t = []  # prediction accuracy using training data\n",
    "yy_v = []  # prediction accuracy using validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iteration\t  training\tvalidation\n",
      "         0\t    0.1212\t    0.1286\n",
      "         1\t    0.1518\t    0.1482\n",
      "         2\t    0.2356\t    0.2376\n",
      "         3\t    0.2896\t    0.2934\n",
      "         4\t    0.3036\t    0.3040\n",
      "     24900\t    0.9948\t    0.9912\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "n_iter   = 25000             # number of iterations\n",
    "n_batch  =    50             # number of images/training batch\n",
    "\n",
    "learning_rate = 1.e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate)\n",
    "\n",
    "xx, yy_t, yy_v = trainModel(model, optimizer, avloss, modelfile,\n",
    "                            train_x, train_t, \n",
    "                            val_x,   val_t,\n",
    "                            n_iter,  n_batch, \n",
    "                            xx, yy_t, yy_v)\n",
    "\n",
    "print(\"\\ndone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m n_batch  \u001b[38;5;241m=\u001b[39m   \u001b[38;5;241m100\u001b[39m             \u001b[38;5;66;03m# number of images/training batch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.e-4\u001b[39m\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), \n\u001b[1;32m      5\u001b[0m                              lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      7\u001b[0m xx, yy_t, yy_v \u001b[38;5;241m=\u001b[39m trainModel(model, optimizer, avloss, modelfile,\n\u001b[1;32m      8\u001b[0m                             train_x, train_t, \n\u001b[1;32m      9\u001b[0m                             val_x,   val_t,\n\u001b[1;32m     10\u001b[0m                             n_iter,  n_batch, xx, yy_t, yy_v)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "n_iter   =  1000             # number of iterations\n",
    "n_batch  =   100             # number of images/training batch\n",
    "learning_rate = 1.e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate)\n",
    "\n",
    "xx, yy_t, yy_v = trainModel(model, optimizer, avloss, modelfile,\n",
    "                            train_x, train_t, \n",
    "                            val_x,   val_t,\n",
    "                            n_iter,  n_batch, xx, yy_t, yy_v)\n",
    "\n",
    "print(\"\\ndone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(xx, yy_t, yy_v):\n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "    \n",
    "    # axis limits\n",
    "    ax.set_ylim((0.9, 1))\n",
    "    ax.set_xlim((xx[0], xx[-1]))\n",
    "    \n",
    "    ax.plot(xx, yy_t, 'b', label='Training')\n",
    "    ax.plot(xx, yy_v, 'r', label='Validation')\n",
    "    \n",
    "    ax.set_title('Training and Validation Accuracy', fontsize=16)\n",
    "    ax.set_xlabel('Iterations', fontsize=16)\n",
    "    ax.set_ylabel('Accuracy', fontsize=16)\n",
    "    ax.grid(True, which=\"both\", linestyle='-')\n",
    "    \n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    plt.savefig('fig_mnist_accuracy.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(xx, yy_t, yy_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct predictions:    99.05%\n"
     ]
    }
   ],
   "source": [
    "# if use_saved_model = False, use current parameters of trained model,\n",
    "# otherwise use model with largest prediction accuracy computed using\n",
    "# validation data.\n",
    "use_saved_model = True\n",
    "if use_saved_model:\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "\n",
    "# remember to restructure inputs as an \n",
    "# (N, C, H, W) = (-1, 1, 28, 28) tensor\n",
    "x = torch.from_numpy(test_x).view(-1, 1, 28, 28)\n",
    "t = torch.from_numpy(test_t)\n",
    "\n",
    "model.eval()\n",
    "y = model(x)\n",
    "a = accuracy(y, t)\n",
    "\n",
    "print('Percentage of correct predictions: %8.2f%s' % (100*a, '%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
